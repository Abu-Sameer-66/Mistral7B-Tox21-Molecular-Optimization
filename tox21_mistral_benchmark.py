# -*- coding: utf-8 -*-
"""tox21_mistral_benchmark.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fwlcjclKEKTZMIMdoHZxcVN3taaYSf0u
"""

# Install dependencies if running in Colab/Notebook:
# !pip install --quiet deepchem rdkit transformers peft bitsandbytes accelerate

import os
import gc
import torch
import pandas as pd
import numpy as np
import deepchem as dc
from rdkit import Chem
from rdkit.Chem.Scaffolds.MurckoScaffold import MurckoScaffoldSmiles
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel, LoraConfig, TaskType, get_peft_model
from sklearn.metrics import roc_auc_score
from collections import defaultdict
import torch.nn.functional as F

# --- 1. SYSTEM SETUP ---
os.environ["CUDA_VISIBLE_DEVICES"] = "0"
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
torch.cuda.empty_cache()
gc.collect()

# --- 2. DATA LOADING & BALANCING ---
print("[*] Loading Tox21 Dataset...")
url = "https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/tox21.csv.gz"
df_raw = pd.read_csv(url)

tox21_tasks = ['NR-AR', 'NR-AR-LBD', 'NR-AhR', 'NR-Aromatase', 'NR-ER', 'NR-ER-LBD',
               'NR-PPAR-gamma', 'SR-ARE', 'SR-ATAD5', 'SR-HSE', 'SR-MMP', 'SR-p53']

# Scaffold Split for scientific validity
print("[*] Performing Scaffold Split...")
valid_smiles = [s for s in df_raw['smiles'].unique() if Chem.MolFromSmiles(s) and MurckoScaffoldSmiles(mol=Chem.MolFromSmiles(s))]
unique_ds = dc.data.NumpyDataset(X=np.zeros(len(valid_smiles)), ids=np.array(valid_smiles))
splitter = dc.splits.ScaffoldSplitter()
train_idx, valid_idx, _ = splitter.split(unique_ds)

train_smiles_set = set(np.array(valid_smiles)[train_idx])
valid_smiles_set = set(np.array(valid_smiles)[valid_idx])

def create_dataset(full_df, smiles_set, is_train=True):
    data = []
    subset = full_df[full_df['smiles'].isin(smiles_set)]
    for _, row in subset.iterrows():
        s = row['smiles']
        for task in tox21_tasks:
            label = row[task]
            if not pd.isna(label):
                prompt = f"Molecule: {s}\nTask: {task}\nIs Toxic: {int(label)}"
                # Handling class imbalance via oversampling (8x for positive class)
                repeat = 8 if (is_train and int(label) == 1) else 1
                for _ in range(repeat):
                    data.append({'prompt': prompt, 'label': int(label), 'task': task})
    return pd.DataFrame(data)

df_train = create_dataset(df_raw, train_smiles_set, is_train=True).sample(frac=1, random_state=42)
train_ds = dc.data.NumpyDataset(X=df_train['prompt'].values, y=df_train['label'].values)
print(f"[*] Training dataset size: {len(df_train)}")

# --- 3. MODEL INITIALIZATION ---
MODEL_ID = "mistralai/Mistral-7B-v0.1"
bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type="nf4", bnb_4bit_compute_dtype=torch.float16)
tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
tokenizer.pad_token = tokenizer.eos_token

print("[*] Loading Model Weights...")
base_model = AutoModelForCausalLM.from_pretrained(MODEL_ID, quantization_config=bnb_config, device_map="auto")
base_model.gradient_checkpointing_enable()

peft_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM, r=64, lora_alpha=128, lora_dropout=0.05,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
)

# Load ZINC pre-trained weights if available (Optional Check)
if os.path.exists("./mistral_zinc_stable"):
    model = PeftModel.from_pretrained(base_model, "./mistral_zinc_stable", is_trainable=True)
    print("[*] Loaded ZINC adapters.")
else:
    model = get_peft_model(base_model, peft_config)

# Fix for gradient checkpointing compatibility
model.enable_input_require_grads()

# --- 4. TRAINING LOOP ---
optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)
print("[*] Starting Training (Target: 5500 Steps)...")

model.train()
for i, (X, y, w, ids) in enumerate(train_ds.iterbatches(batch_size=2)):
    inputs = tokenizer(X.tolist(), padding=True, truncation=True, max_length=256, return_tensors="pt").to("cuda")
    outputs = model(**inputs, labels=inputs["input_ids"])

    loss = outputs.loss

    # Stability Check: Stop if NaN is encountered
    if torch.isnan(loss):
        print(f"[!] NaN loss detected at step {i}. Stopping training.")
        break

    loss.backward()
    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
    optimizer.step()
    optimizer.zero_grad()

    if i % 100 == 0: print(f"Step {i} | Loss: {loss.item():.4f}")

    # Save checkpoints periodically
    if i > 0 and i % 500 == 0:
        model.save_pretrained(f"./mistral_checkpoint_{i}")
        torch.cuda.empty_cache()
    if i >= 5500: # Convergence limit
        break

model.save_pretrained("./mistral_tox21_final")
print("[*] Training Complete.")

# --- 5. EVALUATION ---
print("\n[*] Running Evaluation...")
model.eval()
df_valid = create_dataset(df_raw, valid_smiles_set, is_train=False)
id_0, id_1 = tokenizer.encode("0", add_special_tokens=False)[-1], tokenizer.encode("1", add_special_tokens=False)[-1]
task_probs, task_labels = defaultdict(list), defaultdict(list)

with torch.no_grad():
    for i, row in df_valid.iterrows():
        eval_p = row['prompt'].split("Is Toxic:")[0] + "Is Toxic:"
        inputs = tokenizer(eval_p, return_tensors='pt').to("cuda")
        outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])

        logits = outputs.logits[0, -1, [id_0, id_1]]
        p = F.softmax(logits, dim=-1)[1].item()

        task_probs[row['task']].append(p)
        task_labels[row['task']].append(row['label'])

# --- REPORT ---
print("\n" + "="*50 + "\nRESULTS\n" + "="*50)
final_scores = []
for t in sorted(task_probs.keys()):
    if len(set(task_labels[t])) > 1:
        score = roc_auc_score(task_labels[t], task_probs[t])
        final_scores.append(score)
        print(f"Task: {t:<15} | ROC-AUC: {score:.4f}")

print("-" * 50)
print(f"MEAN ROC-AUC: {np.mean(final_scores):.4f}")
print("="*50)

